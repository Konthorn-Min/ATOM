# -*- coding: utf-8 -*-
"""(Test5) MIRNetv2_kaggle.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1M-ZTWXVHWygfHq4vsFZcGRdh4iJVkGYl
"""

# เช็คเวอรืชั่น tensorflow-addons= กับ transformers กับ python ได้ที่ https://www.tensorflow.org/install/source#gpu
!pip install tensorflow-addons==0.22.0
!pip install TensorFlow==2.14.0
!pip install git+https://github.com/wandb/wandb.git
!yes | apt install --allow-change-held-packages libcudnn=8.7.0-1+cuda11.8
# !apt-get install libcudnn8=8.1.1.33-1+cuda11.2.0.1 --allow-downgrades
import random
import numpy as np
from glob import glob
from functools import partial
from PIL import Image, ImageOps

import matplotlib.pyplot as plt
from mpl_toolkits.axes_grid1 import ImageGrid

import tensorflow as tf
import tensorflow_addons as tfa
from transformers.tf_utils import shape_list

import wandb
from wandb.keras import WandbMetricsLogger, WandbModelCheckpoint


AUTOTUNE = tf.data.AUTOTUNE

!pip show tensorflow_addons

import os
from getpass import getpass
#ใส่ wandb_api_key ของตัวเอง (You can find your API key in your browser here: https://wandb.ai/authorize)
api_key = getpass('Enter your wandb API key:')
os.environ["WANDB_API_KEY"] = api_key
wandb.login()

"""#Setting up a Weights & Biases run
<hr />

**wandb.init() **เป็นฟังก์ชันที่ใช้เพื่อเริ่มต้นการทำงานกับ Weights & Biases (W&B) ในโปรเจ็กต์

*   project: ชื่อของโปรเจ็กต์
*   entity: ชื่อผู้ใช้ (username) หรือทีม (team name)
"""

# ตรวจสอบ TensorFlow เวอร์ชัน
print(f"TensorFlow version: {tf.__version__}")

# การตรวจสอบ GPU
gpus = tf.config.list_physical_devices('GPU')
if not gpus:
    print('No GPUs found. Using CPU instead.')
    strategy = tf.distribute.get_strategy()  # Default to default strategy that works on CPU and single GPU
else:
    print('GPUs found. Using GPU.')
    strategy = tf.distribute.MirroredStrategy()

# กำหนดค่าพารามิเตอร์ WandB
config_defaults = {
    'seed': 42,
    'num_gpus': len(tf.config.list_physical_devices('GPU')),
    'dataset_artifact_address': 'ml-colabs/mirnet-v2/lol-dataset:v0',
    'image_size': 128,
    'max_train_images': 400,
    'batch_size': 4,
    'channels': 80,
    'channel_factor': 1.5,
    'num_mrb_blocks': 2,
    'add_residual_connection': True,
    'initial_learning_rate': 2e-4,
    'minimum_learning_rate': 1e-6,
    'epochs': 100,
}

# เริ่มการทำงาน WandB และใช้ config_defaults
wandb.init(project="mirnet-v2", entity="konthorn", config=config_defaults)
config = wandb.config

#ตั้งค่าขนาด batch_size ตามจำนวน replicas
batch_size = config.batch_size * strategy.num_replicas_in_sync

run = wandb.init(project="mirnet-v2", entity="konthorn")

"""#LOL Dataset<hr />
This dataset provides 485 images for training and 15 for testing.

ที่มา dataset: https://www.kaggle.com/datasets/soumikrakshit/lol-dataset
"""

from google.colab import drive
drive.mount('/content/drive')

# เปลี่ยน path เป็นตำแหน่งที่เก็บข้อมูลที่แน่นอนบน Google Drive
dataset_dir = "/content/drive/My Drive/dataset444/lol_dataset"


TRAIN_LOW_LIGHT_IMAGES = sorted(
    glob(os.path.join(dataset_dir, "our485/low/*"))
)[:config.max_train_images]
TRAIN_GROUND_TRUTH_IMAGES = sorted(
    glob(os.path.join(dataset_dir, "our485/high/*"))
)[:config.max_train_images]

VAL_LOW_LIGHT_IMAGES = sorted(
    glob(os.path.join(dataset_dir, "our485/low/*"))
)[config.max_train_images:]
VAL_GROUND_TRUTH_IMAGES = sorted(
    glob(os.path.join(dataset_dir, "our485/high/*"))
)[config.max_train_images:]

TEST_LOW_LIGHT_IMAGES = sorted(
    glob(os.path.join(dataset_dir, "eval15/low/*"))
)
TEST_GROUND_TRUTH_IMAGES = sorted(
    glob(os.path.join(dataset_dir, "eval15/high/*"))
)

# คำสั่ง print ถูกใช้เพื่อแสดงขนาดของชุดข้อมูลที่จะใช้
print(f"Train Dataset: ({len(TRAIN_LOW_LIGHT_IMAGES)}, {len(TRAIN_GROUND_TRUTH_IMAGES)})")
print(f"Validation Dataset: ({len(VAL_LOW_LIGHT_IMAGES)}, {len(VAL_GROUND_TRUTH_IMAGES)})")
print(f"Test Dataset: ({len(TEST_LOW_LIGHT_IMAGES)}, {len(TEST_GROUND_TRUTH_IMAGES)})")

def plot_results(images, titles, figure_size=(12, 12)):
    """A simple utility for plotting the results"""
    fig = plt.figure(figsize=figure_size)
    for i in range(len(images)):
        fig.add_subplot(1, len(images), i + 1).set_title(titles[i])
        _ = plt.imshow(images[i])
        plt.axis("off")
    plt.show()


for idx, (low_light_image, ground_truth_image) in enumerate(zip(TRAIN_LOW_LIGHT_IMAGES[:7], TRAIN_GROUND_TRUTH_IMAGES[:7])):
    plot_results(
        [Image.open(low_light_image), Image.open(ground_truth_image)],
        [f"Train Low-light Image {idx}", f"Train Ground-truth Image {idx}"]
    )

def read_image(image_path):
    image = tf.io.read_file(image_path)
    image = tf.image.decode_png(image, channels=3)
    image = tf.cast(image, dtype=tf.float32) / 255.0
    return image


def random_crop(low_image, gt_image):
    low_image_shape = tf.shape(low_image)[:2]
    crop_width = tf.random.uniform(
        shape=(), maxval=low_image_shape[1] - config.image_size + 1, dtype=tf.int32
    )
    crop_height = tf.random.uniform(
        shape=(), maxval=low_image_shape[0] - config.image_size + 1, dtype=tf.int32
    )
    low_image_cropped = low_image[
        crop_height : crop_height + config.image_size,
        crop_width : crop_width + config.image_size
    ]
    gt_image_cropped = gt_image[
        crop_height : crop_height + config.image_size,
        crop_width : crop_width + config.image_size
    ]
    low_image_cropped.set_shape([config.image_size, config.image_size, 3])
    gt_image_cropped.set_shape([config.image_size, config.image_size, 3])
    return low_image_cropped, gt_image_cropped


def resize_images(low_image, gt_image):
    low_image = tf.image.resize(low_image, size=[config.image_size, config.image_size])
    gt_image = tf.image.resize(gt_image, size=[config.image_size, config.image_size])
    low_image.set_shape([config.image_size, config.image_size, 3])
    gt_image.set_shape([config.image_size, config.image_size, 3])
    return low_image, gt_image


def load_data(low_light_image_path, enhanced_image_path, apply_resize):
    low_light_image = read_image(low_light_image_path)
    enhanced_image = read_image(enhanced_image_path)
    low_light_image, enhanced_image = (
        resize_images(low_light_image, enhanced_image) if apply_resize
        else random_crop(low_light_image, enhanced_image)
    )
    return low_light_image, enhanced_image


def get_dataset(low_light_images, enhanced_images, apply_resize):
    dataset = tf.data.Dataset.from_tensor_slices((low_light_images, enhanced_images))
    dataset = dataset.map(
        partial(load_data, apply_resize=apply_resize),
        num_parallel_calls=AUTOTUNE
    )
    dataset = dataset.batch(config.batch_size, drop_remainder=True)
    dataset = dataset.prefetch(AUTOTUNE)
    return dataset

train_dataset = get_dataset(TRAIN_LOW_LIGHT_IMAGES, TRAIN_GROUND_TRUTH_IMAGES, apply_resize=False)
val_dataset = get_dataset(VAL_LOW_LIGHT_IMAGES, VAL_GROUND_TRUTH_IMAGES, apply_resize=True)

print("Train Dataset:", train_dataset.element_spec)
print("Validation Dataset:", val_dataset.element_spec)

#Selective Kernel Feature Fusion
class SelectiveKernelFeatureFusion(tf.keras.layers.Layer):
    def __init__(self, channels: int, *args, **kwargs) -> None:
        super().__init__(*args, **kwargs)

        self.hidden_channels = max(int(channels / 8), 4)

        self.average_pooling = tfa.layers.AdaptiveAveragePooling2D(output_size=1)

        self.conv_channel_downscale = tf.keras.layers.Conv2D(
            self.hidden_channels, kernel_size=1, padding="same"
        )
        self.conv_attention_1 = tf.keras.layers.Conv2D(
            channels, kernel_size=1, strides=1, padding="same"
        )
        self.conv_attention_2 = tf.keras.layers.Conv2D(
            channels, kernel_size=1, strides=1, padding="same"
        )

        self.leaky_relu = tf.keras.layers.LeakyReLU(alpha=0.2)
        self.sorftmax = tf.keras.layers.Softmax(axis=-1)

    def call(self, inputs, *args, **kwargs):
        combined_input_features = inputs[0] + inputs[1]
        channel_wise_statistics = self.average_pooling(combined_input_features)
        downscaled_channel_wise_statistics = self.conv_channel_downscale(
            channel_wise_statistics
        )
        attention_vector_1 = self.sorftmax(
            self.conv_attention_1(downscaled_channel_wise_statistics)
        )
        attention_vector_2 = self.sorftmax(
            self.conv_attention_2(downscaled_channel_wise_statistics)
        )
        selected_features = (
            inputs[0] * attention_vector_1 + inputs[1] * attention_vector_2
        )
        return selected_features

#Residual Contextual Block
class ContextBlock(tf.keras.layers.Layer):
    def __init__(self, channels: int, *args, **kwargs):
        super().__init__(*args, **kwargs)

        self.mask_conv = tf.keras.layers.Conv2D(
            1, kernel_size=1, padding="same"
        )

        self.channel_add_conv_1 = tf.keras.layers.Conv2D(
            channels, kernel_size=1, padding="same"
        )
        self.channel_add_conv_2 = tf.keras.layers.Conv2D(
            channels, kernel_size=1, padding="same"
        )

        self.softmax = tf.keras.layers.Softmax(axis=1)
        self.leaky_relu = tf.keras.layers.LeakyReLU(alpha=0.2)

    def modeling(self, inputs):
        _, height, width, channels = shape_list(inputs)
        reshaped_inputs = tf.expand_dims(
            tf.reshape(inputs, (-1, channels, height * width)), axis=1
        )

        context_mask = self.mask_conv(inputs)
        context_mask = tf.reshape(context_mask, (-1, height * width, 1))
        context_mask = self.softmax(context_mask)
        context_mask = tf.expand_dims(context_mask, axis=1)

        context = tf.reshape(
            tf.matmul(reshaped_inputs, context_mask), (-1, 1, 1, channels)
        )
        return context

    def call(self, inputs, *args, **kwargs):
        context = self.modeling(inputs)
        channel_add_term = self.channel_add_conv_1(context)
        channel_add_term = self.leaky_relu(channel_add_term)
        channel_add_term = self.channel_add_conv_2(channel_add_term)
        return inputs + channel_add_term


class ResidualContextBlock(tf.keras.layers.Layer):
    def __init__(self, channels: int, groups: int, *args, **kwargs):
        super().__init__(*args, **kwargs)

        self.conv_1 = tf.keras.layers.Conv2D(
            channels, kernel_size=3, padding="same", groups=groups
        )
        self.conv_2 = tf.keras.layers.Conv2D(
            channels, kernel_size=3, padding="same", groups=groups
        )
        self.leaky_relu = tf.keras.layers.LeakyReLU(alpha=0.2)

        self.context_block = ContextBlock(channels=channels)

    def call(self, inputs):
        x = self.conv_1(inputs)
        x = self.leaky_relu(x)
        x = self.conv_2(x)
        x = self.context_block(x)
        x = self.leaky_relu(x)
        x = x + inputs
        return x

#Multi-Scale Residual Block
class DownBlock(tf.keras.layers.Layer):
    def __init__(
        self, channels: int, channel_factor: float, *args, **kwargs
    ):
        super(DownBlock, self).__init__(*args, **kwargs)
        self.average_pool = tf.keras.layers.AveragePooling2D(
            pool_size=2, strides=2
        )
        self.conv = tf.keras.layers.Conv2D(
            int(channels * channel_factor),
            kernel_size=1,
            strides=1,
            padding="same"
        )

    def call(self, inputs, *args, **kwargs):
        return self.conv(self.average_pool(inputs))


class DownSampleBlock(tf.keras.layers.Layer):
    def __init__(
        self,
        channels: int,
        scale_factor: int,
        channel_factor: float,
        *args,
        **kwargs
    ):
        super(DownSampleBlock, self).__init__(*args, **kwargs)
        self.layers = []
        for _ in range(int(np.log2(scale_factor))):
            self.layers.append(DownBlock(channels, channel_factor))
            channels = int(channels * channel_factor)

    def call(self, x, *args, **kwargs):
        for layer in self.layers:
            x = layer(x)
        return x

class UpBlock(tf.keras.layers.Layer):
    def __init__(self, channels: int, channel_factor: float, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.conv = tf.keras.layers.Conv2D(
            int(channels // channel_factor), kernel_size=1, strides=1, padding="same"
        )
        self.upsample = tf.keras.layers.UpSampling2D(size=2, interpolation="bilinear")

    def call(self, inputs, *args, **kwargs):
        return self.upsample(self.conv(inputs))


class UpSampleBlock(tf.keras.layers.Layer):
    def __init__(
        self, channels: int, scale_factor: int, channel_factor: float, *args, **kwargs
    ):
        super().__init__(*args, **kwargs)
        self.layers = []
        for _ in range(int(np.log2(scale_factor))):
            self.layers.append(UpBlock(channels, channel_factor))
            channels = int(channels // channel_factor)

    def call(self, x, *args, **kwargs):
        for layer in self.layers:
            x = layer(x)
        return x

class MultiScaleResidualBlock(tf.keras.layers.Layer):
    def __init__(
        self,
        channels: int,
        channel_factor: float,
        groups: int,
        *args,
        **kwargs
    ):
        super().__init__(*args, **kwargs)

        # Residual Context Blocks
        self.rcb_top = ResidualContextBlock(
            int(channels * channel_factor**0), groups=groups
        )
        self.rcb_middle = ResidualContextBlock(
            int(channels * channel_factor**1), groups=groups
        )
        self.rcb_bottom = ResidualContextBlock(
            int(channels * channel_factor**2), groups=groups
        )

        # Downsample Blocks
        self.down_2 = DownSampleBlock(
            channels=int((channel_factor**0) * channels),
            scale_factor=2,
            channel_factor=channel_factor,
        )
        self.down_4_1 = DownSampleBlock(
            channels=int((channel_factor ** 0) * channels),
            scale_factor=2,
            channel_factor=channel_factor,
        )
        self.down_4_2 = DownSampleBlock(
            channels=int((channel_factor ** 1) * channels),
            scale_factor=2,
            channel_factor=channel_factor,
        )

        # UpSample Blocks
        self.up21_1 = UpSampleBlock(
            channels=int((channel_factor ** 1) * channels),
            scale_factor=2,
            channel_factor=channel_factor,
        )
        self.up21_2 = UpSampleBlock(
            channels=int((channel_factor ** 1) * channels),
            scale_factor=2,
            channel_factor=channel_factor,
        )
        self.up32_1 = UpSampleBlock(
            channels=int((channel_factor ** 2) * channels),
            scale_factor=2,
            channel_factor=channel_factor,
        )
        self.up32_2 = UpSampleBlock(
            channels=int((channel_factor ** 2) * channels),
            scale_factor=2,
            channel_factor=channel_factor,
        )

        # SKFF Blocks
        self.skff_top = SelectiveKernelFeatureFusion(
            channels=int(channels * channel_factor**0)
        )
        self.skff_middle = SelectiveKernelFeatureFusion(
            channels=int(channels * channel_factor**1)
        )

        # Convolution
        self.conv_out = tf.keras.layers.Conv2D(
            channels, kernel_size=1, padding="same"
        )

    def call(self, inputs, *args, **kwargs):
        x_top = inputs
        x_middle = self.down_2(x_top)
        x_bottom = self.down_4_2(self.down_4_1(x_top))

        x_top = self.rcb_top(x_top)
        x_middle = self.rcb_middle(x_middle)
        x_bottom = self.rcb_bottom(x_bottom)

        x_middle = self.skff_middle([x_middle, self.up32_1(x_bottom)])
        x_top = self.skff_top([x_top, self.up21_1(x_middle)])

        x_top = self.rcb_top(x_top)
        x_middle = self.rcb_middle(x_middle)
        x_bottom = self.rcb_bottom(x_bottom)

        x_middle = self.skff_middle([x_middle, self.up32_2(x_bottom)])
        x_top = self.skff_top([x_top, self.up21_2(x_middle)])

        output = self.conv_out(x_top)
        output = output + inputs

        return output

#Putting Everything Together in MIRNet-v2
class RecursiveResidualGroup(tf.keras.layers.Layer):
    def __init__(
        self,
        channels: int,
        num_mrb_blocks: int,
        channel_factor: float,
        groups: int,
        *args,
        **kwargs
    ):
        super().__init__(*args, **kwargs)
        self.layers = [
            MultiScaleResidualBlock(channels, channel_factor, groups)
            for _ in range(num_mrb_blocks)
        ]
        self.layers.append(
            tf.keras.layers.Conv2D(
                channels, kernel_size=3, strides=1, padding="same"
            )
        )

    def call(self, inputs, *args, **kwargs):
        residual = inputs
        for layer in self.layers:
            residual = layer(residual)
        residual = residual + inputs
        return residual


class MirNetv2(tf.keras.Model):
    def __init__(
        self,
        channels: int,
        channel_factor: float,
        num_mrb_blocks: int,
        add_residual_connection: bool,
        *args,
        **kwargs
    ):
        super().__init__(*args, **kwargs)

        self.add_residual_connection = add_residual_connection

        self.conv_in = tf.keras.layers.Conv2D(
            channels, kernel_size=3, padding="same"
        )

        self.rrg_block_1 = RecursiveResidualGroup(
            channels, num_mrb_blocks, channel_factor, groups=1
        )
        self.rrg_block_2 = RecursiveResidualGroup(
            channels, num_mrb_blocks, channel_factor, groups=2
        )
        self.rrg_block_3 = RecursiveResidualGroup(
            channels, num_mrb_blocks, channel_factor, groups=4
        )
        self.rrg_block_4 = RecursiveResidualGroup(
            channels, num_mrb_blocks, channel_factor, groups=4
        )

        self.conv_out = tf.keras.layers.Conv2D(
            3, kernel_size=3, padding="same"
        )

    def call(self, inputs, training=None, mask=None):
        shallow_features = self.conv_in(inputs)
        deep_features = self.rrg_block_1(shallow_features)
        deep_features = self.rrg_block_2(deep_features)
        deep_features = self.rrg_block_3(deep_features)
        deep_features = self.rrg_block_4(deep_features)
        output = self.conv_out(deep_features)
        output = output + inputs if self.add_residual_connection else output
        return output

# class CharbonnierLoss(tf.keras.losses.Loss):
#     def __init__(self, epsilon: float, *args, **kwargs):
#         super().__init__(*args, **kwargs)
#         self.epsilon = tf.convert_to_tensor(epsilon)

#     def call(self, y_true, y_pred):
#         squared_difference = tf.square(y_true - y_pred)
#         return tf.reduce_mean(
#             tf.sqrt(squared_difference + tf.square(self.epsilon))
#         )

class CharbonnierLoss(tf.keras.losses.Loss):
    def __init__(self, epsilon: float, reduction=tf.keras.losses.Reduction.AUTO, *args, **kwargs):
        super().__init__(reduction=reduction, *args, **kwargs)
        self.epsilon = tf.convert_to_tensor(epsilon)

    def call(self, y_true, y_pred):
        squared_difference = tf.square(y_true - y_pred)
        return tf.reduce_mean(
            tf.sqrt(squared_difference + tf.square(self.epsilon))
        )

class PSNRMetric(tf.keras.metrics.Metric):
    def __init__(self, max_val: float, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.max_val = max_val
        self.psnr = tf.keras.metrics.Mean(name="psnr")

    def update_state(self, y_true, y_pred, *args, **kwargs):
        psnr = tf.image.psnr(y_true, y_pred, max_val=self.max_val)
        self.psnr.update_state(psnr, *args, **kwargs)

    def result(self):
        return self.psnr.result()

    def reset_state(self):
        self.psnr.reset_state()


class SSIMMetric(tf.keras.metrics.Metric):
    def __init__(self, max_val: float, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.max_val = max_val
        self.ssim = tf.keras.metrics.Mean(name="ssim")

    def update_state(self, y_true, y_pred, *args, **kwargs):
        ssim = tf.image.ssim(y_true, y_pred, max_val=self.max_val)
        self.ssim.update_state(ssim, *args, **kwargs)

    def result(self):
        return self.ssim.result()

    def reset_state(self):
        self.ssim.reset_state()

with strategy.scope():
    model = MirNetv2(
        channels=config.channels,
        channel_factor=config.channel_factor,
        num_mrb_blocks=config.num_mrb_blocks,
        add_residual_connection=config.add_residual_connection
    )

    dummy_inputs = tf.ones((1, config.image_size, config.image_size, 3))
    dummy_outputs = model(dummy_inputs)
    model.summary(expand_nested=True)
    print("\nInput Shape:", dummy_inputs.shape)
    print("Output Shape:", dummy_outputs.shape)

    loss = CharbonnierLoss(epsilon=1e-3)

    psnr_metric = PSNRMetric(max_val=1.0)
    ssim_metric = SSIMMetric(max_val=1.0)

    decay_steps = (config.max_train_images // config.batch_size) * config.epochs
    lr_schedule_fn = tf.keras.optimizers.schedules.CosineDecay(
        initial_learning_rate=config.initial_learning_rate,
        decay_steps=decay_steps,
        alpha=config.minimum_learning_rate,
    )
    optimizer = tf.keras.optimizers.Adam(
        learning_rate=lr_schedule_fn, beta_1=0.9, beta_2=0.999,
    )

    model.compile(
        optimizer=optimizer, loss=loss, metrics=[psnr_metric, ssim_metric]
    )

# Commented out IPython magic to ensure Python compatibility.
# %%wandb
# 
# callbacks = [
#     WandbMetricsLogger(),
#     WandbModelCheckpoint(filepath="model", save_best_only=False)
# ]
# # สร้าง Artifact
# artifact = wandb.Artifact("model", type="model")
# 
# # บันทึกโมเดลในรูปแบบ SavedModel
# model.save("model", save_format="tf")
# 
# # เพิ่มไดเรกทอรีที่มีโมเดลไปยัง Artifact
# artifact.add_dir("model")
# 
# # บันทึก Artifact
# run.log_artifact(artifact)
# 
# 
# 
# model.fit(
#     train_dataset,
#     validation_data=val_dataset,
#     epochs=config.epochs,
#     callbacks=callbacks
# )

run_id = "xa8sc2qx"

# รับข้อมูลการเข้าถึง Artifact จากรันที่ระบุ
run = wandb.init(id=run_id, project="mirnet-v2", entity="konthorn")
artifact = run.use_artifact("model:latest")

# ระบุไดเรกทอรีที่คุณต้องการดาวน์โหลดไฟล์ไป
download_dir = "downloaded_files"

# ดาวน์โหลดไฟล์จาก Artifact ไปยังไดเรกทอรีที่ระบุ
artifact.download(download_dir)

def preprocess_image(image):
    """Preprocesses the image for inference.

    Returns:
        A numpy array of shape (1, height, width, 3) preprocessed for inference.
    """
    image = tf.keras.preprocessing.image.img_to_array(image)
    image = image.astype("float32") / 255.0
    return np.expand_dims(image, axis=0)


def postprocess_image(model_output):
    """Postprocesses the model output for inference.

    Returns:
        A list of PIL.Image.Image objects postprocessed for visualization.
    """
    model_output = model_output * 255.0
    model_output = model_output.clip(0, 255)
    image = model_output[0].reshape(
        (np.shape(model_output)[1], np.shape(model_output)[2], 3)
    )
    return Image.fromarray(np.uint8(image))



def infer_and_visualize(low_light_image_file, ground_truth_image_file, model):
    low_light_image = Image.open(low_light_image_file)
    preprocessed_image = preprocess_image(low_light_image)
    model_output = model.predict(preprocessed_image, verbose=0)
    post_processed_image = postprocess_image(model_output)
    plot_results(
        images=[low_light_image, ImageOps.autocontrast(low_light_image), post_processed_image],
        titles=["Low-light Image", "PIL Autocontrast", "MirNetv2 Enhanced"],
        figure_size=(22, 15)
    )

for low_light_image_file, ground_truth_image_file in zip(TEST_LOW_LIGHT_IMAGES, TEST_GROUND_TRUTH_IMAGES):
    infer_and_visualize(low_light_image_file, ground_truth_image_file, model)

def plot_results(images, titles, figure_size):
    num_images = len(images)

    plt.figure(figsize=figure_size)

    for i in range(num_images):
        plt.subplot(1, num_images, i + 1)
        plt.imshow(images[i])
        plt.title(titles[i])
        plt.axis('off')

    plt.show()
for low_light_image_file, ground_truth_image_file in zip(TEST_LOW_LIGHT_IMAGES, TEST_GROUND_TRUTH_IMAGES):
    low_light_image = Image.open(low_light_image_file)
    preprocessed_image = preprocess_image(low_light_image)
    model_output = model.predict(preprocessed_image, verbose=0)
    post_processed_image = postprocess_image(model_output)

    # อ่านภาพจริงจาก TEST_GROUND_TRUTH_IMAGES
    ground_truth_image = Image.open(ground_truth_image_file)

    # พล็อตผลลัพธ์
    plot_results(
        images=[low_light_image, post_processed_image, ground_truth_image],
        titles=["Low-light Image", "MirNetv2 Enhanced", "Hight Image"],
        figure_size=(22, 15)
    )

model.summary()

new_low = "/content/drive/MyDrive/dataset444/L.jpg"
new_low2 = "/content/drive/MyDrive/dataset444/L2.jpg"

new_image = Image.open(new_low)
new_image2 = Image.open(new_low2)
resized_new_image = new_image.resize((128, 128))
resized_new_image2 = new_image2.resize((128, 128))
# แปลงเป็น array และเพิ่มมิติ batch
new_image_array = np.expand_dims(np.array(resized_new_image), axis=0)
new_image_array2 = np.expand_dims(np.array(resized_new_image2), axis=0)
# แสดงขนาดของรูปที่ปรับขนาด
print("Resized New Image Shape:", new_image_array.shape)
print("Resized New Image Shape2:", new_image_array2.shape)

resized_new_image = new_image.resize((128, 128))  # ปรับขนาดเป็น 128x128 พิกเซล
preprocessed_new_image = preprocess_image(resized_new_image)
model_output_new_image = model.predict(preprocessed_new_image, verbose=0)
post_processed_new_image = postprocess_image(model_output_new_image)

# พล็อตผลลัพธ์
plot_results(
    images=[resized_new_image, post_processed_new_image],
    titles=["New Image", "MirNetv2 Enhanced"],
    figure_size=(22, 15)
)

resized_new_image2 = new_image2.resize((128, 128))  # ปรับขนาดเป็น 128x128 พิกเซล
preprocessed_new_image2 = preprocess_image(resized_new_image2)
model_output_new_image2 = model.predict(preprocessed_new_image2, verbose=0)
post_processed_new_image = postprocess_image(model_output_new_image2)

# พล็อตผลลัพธ์
plot_results(
    images=[resized_new_image, post_processed_new_image],
    titles=["New Image", "MirNetv2 Enhanced"],
    figure_size=(22, 15)
)

run_id = "xa8sc2qx"

# รับข้อมูลการเข้าถึง Artifact จากรันที่ระบุ
run = wandb.init(id=run_id, project="mirnet-v2", entity="konthorn")
artifact = run.use_artifact("model:latest")

# ระบุไดเรกทอรีที่คุณต้องการดาวน์โหลดไฟล์ไป
download_dir = "downloaded_files"

# ดาวน์โหลดไฟล์จาก Artifact ไปยังไดเรกทอรีที่ระบุ
artifact.download(download_dir)